# 커넥션을 활용하여 토픽으로 데이터를 재생산하는 방법

기존에 처리된 데이터를 Amazon S3에서 다시 로드하여 Kafka로 재처리하는 방법은 Kafka Connect를 활용하여 데이터를 새로운 Kafka 토픽으로 배치하는 방식입니다. 예를 들어, S3에 저장된 데이터는 JSON 형식으로 보관되었고, 이를 재처리하여 새로운 비즈니스 로직에 활용하고자 할 때 Kafka Connect의 `S3 Source Connector`를 사용하여 Kafka 토픽에 데이터를 배치할 수 있습니다. 이때, 아래의 순서에 따라 설정합니다.

### 1. 새로운 Kafka 토픽 생성

새로운 토픽을 만들어 데이터를 수신할 준비를 합니다. `kafka-topics.sh` 스크립트를 사용하여 토픽을 생성할 수 있습니다:

```bash
bin/kafka-topics.sh --create --topic reprocessed-data --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

위의 명령은 `reprocessed-data`라는 이름의 새 토픽을 생성합니다. `--partitions`와 `--replication-factor`는 필요에 따라 조정할 수 있습니다.

### 2. Kafka Connect S3 Source Connector 설정

Kafka Connect를 통해 S3에서 데이터를 로드하기 위해 Kafka Connect의 S3 Source Connector를 설정합니다. 이 커넥터는 S3 버킷에서 JSON 형식의 데이터를 가져와서 Kafka로 스트리밍합니다.

`connect-distributed.properties` 파일에 연결된 커넥터 설정 파일을 작성하거나, REST API를 통해 S3 Source Connector를 설정할 수 있습니다. 여기에서는 REST API를 통해 설정하는 방법을 보여드립니다.

#### 설정 예제 (REST API 사용)

```bash
curl -X POST -H "Content-Type: application/json" --data '{
  "name": "s3-source-connector",
  "config": {
    "connector.class": "io.confluent.connect.s3.S3SourceConnector",
    "tasks.max": "1",
    "s3.region": "ap-northeast-2",  // 사용할 AWS S3 리전 설정
    "s3.bucket.name": "your-s3-bucket-name",
    "s3.access.key.id": "your-access-key",
    "s3.secret.access.key": "your-secret-key",
    "format.class": "io.confluent.connect.s3.format.json.JsonFormat",
    "topic": "reprocessed-data",
    "partitioner.class": "io.confluent.connect.storage.partitioner.DefaultPartitioner",
    "schema.compatibility": "NONE",
    "s3.poll.interval.ms": "10000"
  }
}' http://localhost:8083/connectors
```

#### 설명:

- `"connector.class": "io.confluent.connect.s3.S3SourceConnector"`: S3 소스 커넥터를 지정합니다.
- `"tasks.max": "1"`: 한 번에 실행할 수 있는 최대 태스크 수입니다.
- `"s3.region": "ap-northeast-2"`: 사용 중인 AWS 리전입니다. 여기서는 서울 리전을 예로 들었습니다.
- `"s3.bucket.name": "your-s3-bucket-name"`: 데이터를 가져올 S3 버킷의 이름입니다.
- `"s3.access.key.id"`와 `"s3.secret.access.key"`: AWS 인증에 필요한 키와 비밀 키입니다. (보안을 위해 IAM 역할을 사용하는 것을 권장)
- `"format.class": "io.confluent.connect.s3.format.json.JsonFormat"`: 데이터 형식을 JSON으로 지정합니다.
- `"topic": "reprocessed-data"`: 데이터를 전송할 Kafka 토픽입니다.
- `"partitioner.class"`: 파티셔닝 전략으로 기본 설정을 사용합니다.
- `"schema.compatibility": "NONE"`: 데이터 스키마 호환성을 설정합니다.
- `"s3.poll.interval.ms": "10000"`: S3 버킷을 폴링하는 간격(밀리초)입니다.

이 설정은 Kafka Connect가 주기적으로 S3 버킷을 확인하고, 새롭게 추가된 JSON 데이터를 `reprocessed-data` 토픽으로 전송하게 합니다.

### 3. 데이터 확인

새로운 토픽에 데이터가 잘 수신되는지 확인하려면 다음과 같은 명령어를 사용할 수 있습니다:

```bash
bin/kafka-console-consumer.sh --topic reprocessed-data --from-beginning --bootstrap-server localhost:9092
```

이 명령어는 `reprocessed-data` 토픽에 전송된 데이터를 처음부터 소비하여 터미널에 출력합니다.

### 4. 주요 고려사항

- **재처리 설정**: 데이터가 정기적으로 S3에 업로드되거나 다른 버킷에 백업된 경우, 각 배치 처리마다 필요한 데이터를 불러올 수 있도록 폴링 간격을 조절해야 합니다.
- **데이터 필터링**: 필요에 따라 데이터 필터링을 추가하여 원하는 데이터만 재처리할 수 있습니다.
- **에러 핸들링**: 커넥터 설정에 오류 처리를 포함하여 손상된 파일이나 잘못된 형식의 데이터가 있을 경우 문제를 확인하고 건너뛸 수 있습니다.

이와 같은 방법으로, 이전에 처리된 데이터를 Kafka 토픽으로 재로드하고, 새로운 비즈니스 로직에 맞추어 데이터를 다시 활용할 수 있습니다.
